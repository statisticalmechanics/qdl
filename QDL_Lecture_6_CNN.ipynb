{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QDL_Lecture_6_CNN.ipynb","provenance":[],"authorship_tag":"ABX9TyPO1Ipue+kgOWbBexZBXMDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# A Quick Guide to Deep Learning with Python\n","\n","Kai Zhang, Duke Kunshan University, 2022"],"metadata":{"id":"ToTjY0GpJL5C"}},{"cell_type":"markdown","source":["# Lecture 6 Convolutional Neural Network (CNN or ConvNet)\n","\n","**References**:\n","\n","[Wikipedia] https://en.wikipedia.org/wiki/Convolution\n","\n","[Amidi] https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n","\n","[Fchollet] https://keras.io/examples/vision/mnist_convnet/\n","\n","[Jordan] https://www.jeremyjordan.me/convnet-architectures/\n","\n","[Dumoulin] https://arxiv.org/abs/1603.07285"],"metadata":{"id":"ZhJg6cSnLILA"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import math"],"metadata":{"id":"UEIXdtWcLCT-","executionInfo":{"status":"ok","timestamp":1653010989229,"user_tz":240,"elapsed":9,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#X_train = np.rint(train_images.reshape(60000, 28*28).astype(np.float32) / 255)\n","#X_test = np.rint(test_images.reshape(10000, 28*28).astype(np.float32) / 255)"],"metadata":{"id":"nRTDOH3YLeys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Convolutional layer\n","\n","**receptive field**\n","\n","**weight sharing**\n","\n","Advantages of CNN\n","\n","* translation-invariant\n","* spatial hierarchies of patterns\n","\n","<figure>\n"," <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Comparison_convolution_correlation.svg/1280px-Comparison_convolution_correlation.svg.png\"  width = \"600\" >\n"," <figcaption align=\"left\">Figure 1. Visual comparison of convolution, cross-correlation, and autocorrelation (Wikipedia). \n"," </figcaption>\n","</figure>\n","\n","**convolution**\n","\\begin{equation}\n","(f*g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t - \\tau) d\\tau =\\int_{-\\infty}^{\\infty}  f(t-\\tau) g(\\tau) d\\tau = (g*f)(t)\n","\\end{equation}\n","\n","**cross-correlation**\n","\\begin{equation}\n","(f \\star g)(t) = \\int_{-\\infty}^{\\infty} f(\\tau) g(t + \\tau) d\\tau =(g \\star f)(-t) \\ne \\int_{-\\infty}^{\\infty} f(t+\\tau) g(\\tau) d\\tau =(g \\star f)(t)\n","\\end{equation}\n","\n","\n","\n","rank-3 **tensor**\n","\n","**input layer**: size = [input_height, input_width, input_depth (color channel)]\n","\n","**(convolution) kernel** (**filter**) (patch):  size = [kernel_size, kernel_size, input_depth, output_depth (filter number)]\n","\n","**output layer**: size = [output_height, output_width, output_depth]\n","\n","**feature map** size = [height, width, 1]\n","\n","**stride**\n","\n","**padding**\n","* same: pad with zeros to make the same size as input feature map\n","* valid (default): no padding such that kernel windows do not go outside the boundary of input feature map.\n","\n","\\begin{equation}\n","output\\_width = \\left\\lfloor \\frac{input\\_width + 2\\times padding\\_size - kernel\\_size}{stride} \\right\\rfloor + 1\n","\\end{equation}\n","\n","Number of parameters in each convolutional layer\n","\\begin{equation}\n","parameter\\_number = (kernel\\_size\\times kernel\\_size\\times input\\_depth + 1) \\times output\\_depth\n","\\end{equation}\n","where \"$+1$\" is for the bias.\n"],"metadata":{"id":"x0eyeRTtLufn"}},{"cell_type":"markdown","source":["# Pooling layer\n","\n","To downsample feature maps (usually size is halved). No parameters are needed.\n","\n","* max pooling\n","* average pooling\n","\n","\n","<figure>\n"," <img src=\"https://www.jeremyjordan.me/content/images/2018/04/AlexNet-CNN-architecture-layers.png\"  width = \"800\" >\n"," <figcaption align=\"left\">Figure 2. Architecture for ImageNet Classification with Deep Convolutional Neural Networks. \n"," </figcaption>\n","</figure>\n","\n","<figure>\n"," <img src=\"https://www.jeremyjordan.me/content/images/2018/04/vgg16.png\"  width = \"800\" >\n"," <figcaption align=\"left\">Figure 3. Architecture for Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG16). \n"," </figcaption>\n","</figure>"],"metadata":{"id":"wJh8cI-CZVaH"}},{"cell_type":"markdown","source":["# Keras implementation for MNIST problem"],"metadata":{"id":"m1uoYSPXkatr"}},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"S_Okw3n5MAZk","executionInfo":{"status":"ok","timestamp":1653014389539,"user_tz":240,"elapsed":3045,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"metadata":{"id":"MBzHbSRhl2cr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653014393142,"user_tz":240,"elapsed":557,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"61e140c4-171a-4691-f909-a2f7705889b3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# Scale images to the [0, 1] range\n","x_train = x_train.astype(\"float32\") / 255\n","x_test = x_test.astype(\"float32\") / 255\n","print(\"x_train shape:\", x_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGMoYlz4lsKH","executionInfo":{"status":"ok","timestamp":1653014401162,"user_tz":240,"elapsed":545,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"014ab90a-edf4-4ce5-b7d7-bc5be7faeec0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (60000, 28, 28)\n"]}]},{"cell_type":"code","source":["# Make sure images have shape (28, 28, 1)\n","x_train = np.expand_dims(x_train, -1)\n","x_test = np.expand_dims(x_test, -1)\n","print(\"x_train shape:\", x_train.shape)\n","print(x_train.shape[0], \"train samples\")\n","print(x_test.shape[0], \"test samples\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4mM0YxGl4Ph","executionInfo":{"status":"ok","timestamp":1653014408450,"user_tz":240,"elapsed":141,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"b03b194f-90f6-4a33-f059-6cea01882bbe"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n"]}]},{"cell_type":"code","source":["# Model / data parameters\n","num_classes = 10\n","input_shape = (28, 28, 1)\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","print(y_train[0].shape, y_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZZv10rbmM3N","executionInfo":{"status":"ok","timestamp":1653014475800,"user_tz":240,"elapsed":152,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"5e5f46b6-2ca3-4ab3-dbc0-3d8a45929430"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(10,) [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"]}]},{"cell_type":"code","source":["model = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"softmax\"),\n","    ]\n",")\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zy8ieAzGmZpD","executionInfo":{"status":"ok","timestamp":1653014612103,"user_tz":240,"elapsed":518,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"ce57db50-79d7-4aec-8342-2e72a397161b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 1600)              0         \n","                                                                 \n"," dropout (Dropout)           (None, 1600)              0         \n","                                                                 \n"," dense (Dense)               (None, 10)                16010     \n","                                                                 \n","=================================================================\n","Total params: 34,826\n","Trainable params: 34,826\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Note that this CNN has way more less parameters than fully connected NN."],"metadata":{"id":"580sCIqUpt_S"}},{"cell_type":"code","source":["batch_size = 128\n","epochs = 15\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8wKaba4mnX6","executionInfo":{"status":"ok","timestamp":1653015489556,"user_tz":240,"elapsed":622823,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"a10c1f61-aab0-4267-9d79-446e319a7482"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","422/422 [==============================] - 42s 97ms/step - loss: 0.3829 - accuracy: 0.8829 - val_loss: 0.0849 - val_accuracy: 0.9777\n","Epoch 2/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.1138 - accuracy: 0.9654 - val_loss: 0.0578 - val_accuracy: 0.9837\n","Epoch 3/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0832 - accuracy: 0.9746 - val_loss: 0.0483 - val_accuracy: 0.9857\n","Epoch 4/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0702 - accuracy: 0.9786 - val_loss: 0.0433 - val_accuracy: 0.9888\n","Epoch 5/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0623 - accuracy: 0.9810 - val_loss: 0.0428 - val_accuracy: 0.9882\n","Epoch 6/15\n","422/422 [==============================] - 40s 96ms/step - loss: 0.0583 - accuracy: 0.9823 - val_loss: 0.0377 - val_accuracy: 0.9893\n","Epoch 7/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0527 - accuracy: 0.9834 - val_loss: 0.0381 - val_accuracy: 0.9907\n","Epoch 8/15\n","422/422 [==============================] - 41s 97ms/step - loss: 0.0492 - accuracy: 0.9844 - val_loss: 0.0341 - val_accuracy: 0.9892\n","Epoch 9/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0452 - accuracy: 0.9860 - val_loss: 0.0313 - val_accuracy: 0.9918\n","Epoch 10/15\n","422/422 [==============================] - 40s 96ms/step - loss: 0.0414 - accuracy: 0.9869 - val_loss: 0.0306 - val_accuracy: 0.9913\n","Epoch 11/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0399 - accuracy: 0.9873 - val_loss: 0.0318 - val_accuracy: 0.9908\n","Epoch 12/15\n","422/422 [==============================] - 40s 96ms/step - loss: 0.0377 - accuracy: 0.9881 - val_loss: 0.0327 - val_accuracy: 0.9918\n","Epoch 13/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0364 - accuracy: 0.9882 - val_loss: 0.0278 - val_accuracy: 0.9922\n","Epoch 14/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 0.0310 - val_accuracy: 0.9930\n","Epoch 15/15\n","422/422 [==============================] - 41s 96ms/step - loss: 0.0332 - accuracy: 0.9892 - val_loss: 0.0282 - val_accuracy: 0.9927\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9659d2b390>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["score = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Test loss:\", score[0])\n","print(\"Test accuracy:\", score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Y24PhJ_nXOr","executionInfo":{"status":"ok","timestamp":1653015686146,"user_tz":240,"elapsed":2568,"user":{"displayName":"Kai Zhang","userId":"07064777406155612696"}},"outputId":"18b9fbd3-b096-4fe0-e107-01ca94fbff4b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.02420741319656372\n","Test accuracy: 0.9919000267982483\n"]}]},{"cell_type":"markdown","source":["# Exercise: analyze the architecture and parameters of above CNN for MNIST"],"metadata":{"id":"eEk8yGv5ngpC"}},{"cell_type":"code","source":[""],"metadata":{"id":"5duw_bQDnvLl"},"execution_count":null,"outputs":[]}]}